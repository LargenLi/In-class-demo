[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Comparing Interpretation Methods for Feature Importance",
    "section": "",
    "text": "1 Introduction\n“How do different interpretation methods for tree-based models—Gini importance, permutation-based metrics, and partial dependence plots—compare in identifying the most influential features in customer churn prediction?”\nInterpreting machine learning models is important for creating trust and informing data-driven decision-making in applications such as churn prediction when knowing why customers are churning is as valuable as knowing which customers are going to churn.\nThis project explore how diferent interpretation methods order feature importance in tree-based models using a data on customer churn. Although models such as decision trees and random forests are interpretable by nature, the measurement of importance might differ significantly.\nI contrast four techniques: Gini importance, Accuracy as well as ROC AUC-based permutation importance, and PDP-based importance. Each of the techniques highlights a different facet—everything from contribution to the underlying structure to prediction accuracy or probability ranking impact to marginal effect visualization. Applying these to the same model, I hope to establish their strengths and weaknesses as well as how their results compare or differ.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#data",
    "href": "index.html#data",
    "title": "Comparing Interpretation Methods for Feature Importance",
    "section": "1.1 Data:",
    "text": "1.1 Data:\n\nSource: Telco Customer Churn (https://www.kaggle.com/datasets/abdallahwagih/telco-customer-churn)\nIncludes customer demographics, subscription information, churn data, churn reasons.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "DA.html",
    "href": "DA.html",
    "title": "2  Data Cleaning & Feature Engineering",
    "section": "",
    "text": "2.1 Data Import and Initial Filtering\nCode\ndf = read_excel(\"Telco_customer_churn.xlsx\")\nCode\ncolnames(df) = gsub(\" \", \"_\", tolower(colnames(df)))\ndf = df |&gt; select(-c('churn_reason', 'customerid', 'count', 'country', 'city', 'zip_code', 'lat_long', 'latitude', 'longitude', 'churn_label', 'churn_score'))\nColumn names were standardized to lowercase with underscores. Irrelevant or redundant columns were removed, including identifiers (e.g., customerid), geolocation (e.g., latitude, longitude), and derived churn labels (e.g., churn_label). Since only churned row would have churn reason, the churn_reason is also dropped.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Cleaning & Feature Engineering</span>"
    ]
  },
  {
    "objectID": "DA.html#missing-value",
    "href": "DA.html#missing-value",
    "title": "2  Data Cleaning & Feature Engineering",
    "section": "2.2 Missing value",
    "text": "2.2 Missing value\n\n\nCode\nmissing_summary = sapply(df, function(x) sum(is.na(x)))\nprint(missing_summary)\n\n\n            state            gender    senior_citizen           partner \n                0                 0                 0                 0 \n       dependents     tenure_months     phone_service    multiple_lines \n                0                 0                 0                 0 \n internet_service   online_security     online_backup device_protection \n                0                 0                 0                 0 \n     tech_support      streaming_tv  streaming_movies          contract \n                0                 0                 0                 0 \npaperless_billing    payment_method   monthly_charges     total_charges \n                0                 0                 0                11 \n      churn_value              cltv \n                0                 0 \n\n\nCode\ndf = na.omit(df)\n\n\nRows with missing values are dropped.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Cleaning & Feature Engineering</span>"
    ]
  },
  {
    "objectID": "DA.html#feature-engineering",
    "href": "DA.html#feature-engineering",
    "title": "2  Data Cleaning & Feature Engineering",
    "section": "2.3 Feature Engineering",
    "text": "2.3 Feature Engineering\n\n\nCode\n# Average monthly charge\ndf = df |&gt;\n  mutate(avg_monthly_charge = ifelse(tenure_months == 0, 0, total_charges / tenure_months))\n\n# Multiple services\ndf = df |&gt;\n  mutate(multiple_services = ifelse((phone_service == \"Yes\" & internet_service != \"No\"), 1, 0))\n\n\nTwo new features (Average monthly charge & Multiple services) are created.\nThe variable avg_monthly_charge was created by dividing total_charges by tenure_months. This provides a time-normalized view of how much a customer spends on average per month.\nAnd multiple_services flags customers who are subscribed to both phone and internet services. This binary indicator captures service bundling, which is often a sign of higher customer commitment.\nSince this study is mainly focus on tree-based models and those models can handle multicollinearity naturally, I did not drop the original features.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Cleaning & Feature Engineering</span>"
    ]
  },
  {
    "objectID": "DA.html#binary-encoding-of-yesno-columns",
    "href": "DA.html#binary-encoding-of-yesno-columns",
    "title": "2  Data Cleaning & Feature Engineering",
    "section": "2.4 Binary Encoding of Yes/No Columns",
    "text": "2.4 Binary Encoding of Yes/No Columns\n\n\nCode\n# Columns with 'Yes' and 'No' value.\ndf = df |&gt;\n  mutate(across(where(is.character), ~ ifelse(str_starts(., \"No\"), \"No\", .)))\n\ncolnames(df) = str_trim(colnames(df))\n\nyes_no_cols = df |&gt;\n  select(where(is.character)) |&gt;\n  select(where(~ all(.x %in% c(\"Yes\", \"No\")))) |&gt;\n  colnames()\n\ndf = df |&gt;\n  mutate(across(all_of(yes_no_cols), ~ ifelse(. == \"Yes\", 1, 0)))\n\n\nTo prepare for models in the later section, all “No internet” or “No phone” values were simplified to “No”. Then, binary columns with “Yes”/“No” values were converted to 1/0 format.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Cleaning & Feature Engineering</span>"
    ]
  },
  {
    "objectID": "DA.html#factor-encoding-and-dummy-variables",
    "href": "DA.html#factor-encoding-and-dummy-variables",
    "title": "2  Data Cleaning & Feature Engineering",
    "section": "2.5 Factor Encoding and Dummy Variables",
    "text": "2.5 Factor Encoding and Dummy Variables\n\n\nCode\ndf = df |&gt;\n  mutate(across(where(is.character), as.factor))\n\ndf = df |&gt;\n  select(where(~ n_distinct(.) &gt; 1))\n\ndummies = dummyVars(~ ., data = df)\ndf_encoded = predict(dummies, newdata = df) |&gt; as.data.frame()\n\n\nRemaining categorical variables were converted to factors. Features with only one unique level were removed. All multi-class categorical variables were one-hot encoded using caret::dummyVars.\n\n\nCode\nnumeric_cols_to_scale = df_encoded |&gt;\n  select(where(is.numeric)) |&gt;\n  select(where(~ n_distinct(.) &gt; 2)) |&gt;\n  colnames()\n\n\npreproc = preProcess(df_encoded[, numeric_cols_to_scale], method = c(\"center\", \"scale\"))\nscaled_numeric = predict(preproc, df_encoded[, numeric_cols_to_scale])\n\ndf_final = df_encoded\ndf_final[, numeric_cols_to_scale] = scaled_numeric",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Cleaning & Feature Engineering</span>"
    ]
  },
  {
    "objectID": "DA.html#save-the-processed-dataset",
    "href": "DA.html#save-the-processed-dataset",
    "title": "2  Data Cleaning & Feature Engineering",
    "section": "2.6 Save the Processed Dataset",
    "text": "2.6 Save the Processed Dataset\n\n\nCode\n# write.csv(df_final, \"scaled_telco_data.csv\", row.names = FALSE)\n# write.csv(df_encoded, \"cleaned_telco_data.csv\", row.names = FALSE)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Cleaning & Feature Engineering</span>"
    ]
  },
  {
    "objectID": "Models.html",
    "href": "Models.html",
    "title": "3  Tree-Based Models & Variable Importance",
    "section": "",
    "text": "3.1 Data loaded\nCode\ndf = read_csv(\"cleaned_telco_data.csv\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tree-Based Models & Variable Importance</span>"
    ]
  },
  {
    "objectID": "Models.html#decision-tree-model",
    "href": "Models.html#decision-tree-model",
    "title": "3  Tree-Based Models & Variable Importance",
    "section": "3.2 Decision Tree Model",
    "text": "3.2 Decision Tree Model\n\n\nCode\ntree_model = rpart(churn_value ~ ., data = df, method = \"class\", cp = 0.01)\n\nrpart.plot(tree_model, extra = 1)\n\n\n\n\n\n\n\n\n\nSince this study is would be mainly focused on the variable importance of tree-based model, I start by fitting the dataset by a single decision tree model.\nDue to the algorithm, the single decision tree offers a highly interpretable model where feature importance is directly observable from the tree structure. Features used near the top of the tree typically split the data into the largest and most distinct groups, and are thus considered the most important. In this case, we have the top features would be contract.Month-to-month, internet_service.Fiber optic, and tenure_month. Unexpectedly, none of the include the any information about the amount of money charged.\nHowever, single decision tree model is very likely overfitting the training set, especially when we have 30 features in our dataset. There is a big chance that important features are ignored. Since the a little change in the training set can lead to a very different tree structure, the variance importance observed from a single tree model is very unstable.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tree-Based Models & Variable Importance</span>"
    ]
  },
  {
    "objectID": "Models.html#random-forest-model",
    "href": "Models.html#random-forest-model",
    "title": "3  Tree-Based Models & Variable Importance",
    "section": "3.3 Random Forest Model",
    "text": "3.3 Random Forest Model\n\n\nCode\n# Fit a Random Forest model\nset.seed(123)\ndf$churn_value = as.factor(df$churn_value)\n\ncolnames(df) = make.names(colnames(df))\nrf_model = randomForest(churn_value ~ ., data = df, importance = TRUE)\n\n\n\n\nCode\n# Get Gini importance from the RF model\ngini_df = as.data.frame(importance(rf_model)) |&gt;\n  rownames_to_column(\"Feature\") |&gt;\n  arrange(desc(MeanDecreaseGini))  # sort by importance\n\n\nggplot(gini_df, aes(x = reorder(Feature, MeanDecreaseGini), y = MeanDecreaseGini)) +\n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  labs(\n    title = \"Variable Importance (Gini Index)\",\n    x = \"Feature\",\n    y = \"Mean Decrease in Gini\"\n  ) +\n  theme_minimal(base_size = 12)\n\n\n\n\n\n\n\n\n\nIn order to make the result more stable and reliable, I fit the data with a random forest model. Similar to decision tree, random Forests is also an an intrinsically interpretable model. It’s feature importance using the Mean Decrease in Gini Index, which reflects how much each feature contributes to improving node purity across all trees.\nThis graph presents the Gini-based variable importance from random forest model. Unlike the the single tree model, most of all of top features of random forest are continuous and monetary-derived. This is expected because these features directly reflect the customer’s economic relationship with the business, which is often the strongest signal of satisfaction and risk.\nOther than top 5 features that are monetary and continuous, contract.Month-to-month, internet_service.Fiber optic (the top 2 features of single tree model) have highest mean decrease in Gini.\n\n3.3.1 Partial Dependence Plot\n\n\nCode\ntrain_x = df[, setdiff(names(df), \"churn_value\")]\n\nvar = colnames(train_x)\n\npdp_df = map(var, function(x)partial(rf_model,pred.var = x))|&gt;\n  map_dfr(pivot_longer, cols =1)\n\npdp_importance &lt;- pdp_df %&gt;%\n  group_by(name) %&gt;%\n  summarise(pdp_sd = sd(yhat), .groups = \"drop\")\n\n\npdp_df &lt;- pdp_df %&gt;%\n  left_join(pdp_importance, by = \"name\") %&gt;%\n  arrange(desc(pdp_sd))\n\n\n\n\nCode\n# Step 5: Plot\nggplot(pdp_df, aes(x = value, y = yhat)) +\n  geom_line(color = \"steelblue\") +\n  facet_wrap(~ name, scales = \"free_x\") +\n  labs(\n    title = \"Partial Dependence Plots\",\n    x = \"Feature Value\",\n    y = \"Predicted Churn Probability\"\n  ) +\n  theme_minimal(base_size = 12)\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(pdp_importance, aes(x = reorder(name, pdp_sd), y = pdp_sd)) +\n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  labs(\n    title = \"PDP Importance\",\n    x = \"Features\",\n    y = \"sd(yhat)\"\n  ) +\n  theme_minimal(base_size = 12)\n\n\n\n\n\n\n\n\n\nTo better understand the effect of each feature on predicted churn probability, I apply the Partial Dependence Plots to the random forest model. PDP visualize how the predicted outcome changes as a single feature varies, while all others are held constant. To compare the variable importance with PDP method, I visualize each features by their standard deviation of predicted probabilities (yhat).\nSimilar to the single tree model, the top 2 features are contract.Month-to-month, internet_service.Fiber optic.\nBut surprisingly, the top 15 features are all binary categorical features. I realized that for categorical variables (especially binary), the PDP can exaggerate importance due to discrete jumps in predictions.\n\n\nCode\nis_binary &lt;- function(x) length(unique(x)) == 2 && all(unique(x) %in% c(0, 1))\n\ncontinuous_vars &lt;- train_x %&gt;%\n  select(where(~ !is_binary(.))) %&gt;%\n  colnames()\n\ncon_df = map(continuous_vars, function(x)partial(rf_model,pred.var = x))|&gt;\n  map_dfr(pivot_longer, cols =1)\n\ncon_importance &lt;- con_df %&gt;%\n  group_by(name) %&gt;%\n  summarise(pdp_sd = sd(yhat), .groups = \"drop\")\n\n# Step 4: Join and reorder factor levels\ncon_df &lt;- con_df %&gt;%\n  left_join(con_importance, by = \"name\") %&gt;%\n  arrange(desc(pdp_sd))\n\n\n\n\nCode\n# Step 5: Plot\nggplot(con_df, aes(x = value, y = yhat)) +\n  geom_line(color = \"steelblue\") +\n  facet_wrap(~ name, scales = \"free_x\") +\n  labs(\n    title = \"Partial Dependence Plots (continuous features)\",\n    x = \"Feature Value\",\n    y = \"Predicted Churn Probability\"\n  ) +\n  theme_minimal(base_size = 12)\n\n\n\n\n\n\n\n\n\nCode\nggplot(con_importance, aes(x = reorder(name, pdp_sd), y = pdp_sd)) +\n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  labs(\n    title = \"PDP Importance (continuous features)\",\n    x = \"Features\",\n    y = \"sd(yhat)\"\n  ) +\n  theme_minimal(base_size = 12)\n\n\n\n\n\n\n\n\n\nThen, I conducted a separate PDP analysis for continuous features only to avoid distortion by binary features. I noticed that cltv has higher importance than other monetary features, which is very different from the result of the gini based importance. I was wondering maybe this is because those monetary features are correlated. And PDP’s results might be mislead by multicollinarity.\n\n\nCode\nct_df0 = df|&gt; select(-c(\"tenure_months\", \"monthly_charges\", \"avg_monthly_charge\"))\n\nct_rf = randomForest(churn_value ~ ., data = ct_df0, importance = TRUE)\n  \nct_var = c(\"cltv\", \"total_charges\")\n\nct_df = map(ct_var, function(x)partial(ct_rf, pred.var = x))|&gt;\n  map_dfr(pivot_longer, cols =1)\n\nct_importance &lt;- ct_df %&gt;%\n  group_by(name) %&gt;%\n  summarise(pdp_sd = sd(yhat), .groups = \"drop\")\n\nggplot(ct_df, aes(x = value, y = yhat)) +\n  geom_line(color = \"steelblue\") +\n  facet_wrap(~ name, scales = \"free_x\") +\n  labs(\n    title = \"Partial Dependence Plots (cltv & total_charges)\",\n    x = \"Feature Value\",\n    y = \"Predicted Churn Probability\"\n  ) +\n  theme_minimal(base_size = 12)\n\n\n\n\n\n\n\n\n\nCode\nggplot(ct_importance, aes(x = reorder(name, pdp_sd), y = pdp_sd)) +\n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  labs(\n    title = \"PDP Importance (cltv & total_charges)\",\n    x = \"Features\",\n    y = \"sd(yhat)\"\n  ) +\n  theme_minimal(base_size = 12)\n\n\n\n\n\n\n\n\n\nTherefore, I drop all the correlated monetary features expected total_charges (with the highest Gini-based variable importance), then fit another model and apply PDP with this dataset. Although, the sd(yhat) of total_charges almost doubled but it is still much less than the sd(yhat) of cltv. Maybe other than multicollinearity and binary features, there are something else causing the large difference between PDP and Gini-based importance.\n\n\n3.3.2 Permutation-Based Variable Importance\nTo better handle the binary features, I used permutation-based methods to compute variable importance. Permutation importance are measured by shuffling the values of a feature and see how much does it harm the model’s predictive performance. Since the predicted varible is binary and imbalanced, I decided to use “accuracy” and “roc_auc” as my metric.\n\n\nCode\ntrain_x = df[, setdiff(names(df), \"churn_value\")]\n\n# Define prediction wrapper to return predicted class labels\npred_class = function(object, newdata) {\n  factor(predict(object, newdata = newdata, type = \"response\"), levels = c(0, 1))\n}\n\n# Generate VIP plot using accuracy\nvip(rf_model,\n    method = \"permute\",\n    train = train_x,                \n    target = df$churn_value,           \n    metric = \"accuracy\",               \n    pred_wrapper = pred_class,        \n    nsim = 10,                  \n    num_features = ncol(train_x))     \n\n\n\n\n\n\n\n\n\nThe “tenure_months”, “contract.Month.to.month”, “total_charges”, “internet_service.Fiber.optic”, and “monthly_charges” are the top 5 most important predictors of the “accuracy”. These features cause the greatest drop in model accuracy when they are shuffled. Interestingly, the top 5 features this time include both continous and binary features. And the thoes 5 features are in the top 5 feature list of either Gini of PDP.\n\n\nCode\n# Generate VIP plot using roc\nvip(rf_model,\n    method = \"permute\",\n    train = train_x,\n    target = df$churn_value,\n    metric = \"roc_auc\",\n    pred_wrapper = function(object, newdata) {\n      predict(object, newdata = newdata, type = \"prob\")[, \"1\"]\n    },\n    nsim = 10,\n    num_features = ncol(train_x))\n\n\n\n\n\n\n\n\n\nThe x-axis of the “roc_auc” graph is negative. It is counter intuitive because how can the model performace improves after shuffling the values of feature.\nAccording to instruction book of the vip Package, “roc_auc” metric would treat one of outcomes of the predicted as the “event” of interest. And the default event level is always the first class label in alphabetical or numerical order. So set event_level to “second” would flip graph and solve the problem.\n\n\nCode\n# Generate VIP plot using roc\nvip(rf_model,\n    method = \"permute\",\n    train = train_x,\n    target = df$churn_value,\n    metric = \"roc_auc\",\n    event_level = \"second\",\n    pred_wrapper = function(object, newdata) {\n      predict(object, newdata = newdata, type = \"prob\")[, \"1\"]\n    },\n    nsim = 10,\n    num_features = ncol(train_x))\n\n\n\n\n\n\n\n\n\nThe features like “tenure_month” and “total_charges” still have very high importance under “roc_auc” metric. But the importance of key binary features in other methods (“contract.Month.to.month”, “internet_service.Fiber.optic”) decrease. This difference is because accuracy metric measures how frequently the predictions are correct, so features that improve threshold-based classification would be considered as more important. However, “roc_auc” measures how the model ranks the churners higher than the non-churners, so features that improve overall risk differentiation would be considered as more important, even if they do not impact binary decisions directly.\n\n\nCode\n# ---- Gini Importance ----\ngini_df = as.data.frame(importance(rf_model)) |&gt;\n  rownames_to_column(\"Feature\") |&gt;\n  select(Feature, Gini = MeanDecreaseGini)\n\n# ---- ROC AUC Permutation ----\nroc_vi = vi(\n  rf_model,\n  method = \"permute\",\n  train = train_x,\n  target = df$churn_value,\n  metric = \"roc_auc\",\n  event_level = \"second\",\n  pred_wrapper = function(object, newdata) {\n    predict(object, newdata = newdata, type = \"prob\")[, \"1\"]\n  },\n  nsim = 10\n) |&gt; select(Feature = Variable, ROC_AUC = Importance)\n\n# ---- Accuracy Permutation ----\npred_class = function(object, newdata) {\n  factor(predict(object, newdata, type = \"response\"), levels = c(0, 1))\n}\n\nacc_vi = vi(\n  rf_model,\n  method = \"permute\",\n  train = train_x,\n  target = df$churn_value,\n  metric = \"accuracy\",\n  pred_wrapper = pred_class,\n  nsim = 10\n) |&gt; select(Feature = Variable, Accuracy = Importance)\n\n# ---- Merge All ----\nvi_comparison = reduce(list(gini_df, roc_vi, acc_vi), full_join, by = \"Feature\") |&gt;\n  arrange(desc(Gini))\n\n# ---- Plot ----\nvi_long = vi_comparison |&gt;\n  pivot_longer(cols = -Feature, names_to = \"Metric\", values_to = \"Importance\")\n\nggplot(vi_long, aes(x = reorder(Feature, Importance), y = Importance)) +\n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  facet_wrap(~ Metric, scales = \"free_x\") +\n  labs(\n    title = \"Feature Importance by Metric\",\n    x = \"Feature\",\n    y = \"Importance\"\n  ) +\n  theme_minimal(base_size = 12)\n\n\n\n\n\n\n\n\n\n\n\nCode\nvi_normalized &lt;- vi_comparison |&gt;\n  mutate(across(-Feature, ~ (. - min(.)) / (max(.) - min(.)), .names = \"{.col}_norm\"))\n\n# ---- Compute Correlation Matrix on Normalized Scores ----\nvi_corr_normalized &lt;- vi_normalized |&gt;\n  select(ends_with(\"_norm\")) |&gt;\n  setNames(c(\"Gini\", \"ROC_AUC\", \"Accuracy\")) |&gt;  # Optional: clean up column names\n  cor(use = \"complete.obs\")\n\n# ---- Convert to Long Format for Plotting ----\nvi_corr_long_norm &lt;- reshape2::melt(vi_corr_normalized)\n\n# ---- Plot Normalized Heatmap ----\nggplot(vi_corr_long_norm, aes(x = Var1, y = Var2, fill = value)) +\n  geom_tile(color = \"white\") +\n  geom_text(aes(label = round(value, 2)), color = \"black\", size = 5) +\n  scale_fill_gradient2(low = \"steelblue\", high = \"darkred\", mid = \"white\",\n                       midpoint = 0.5, limit = c(0, 1), space = \"Lab\",\n                       name = \"Correlation\") +\n  theme_minimal(base_size = 14) +\n  labs(title = \"Correlation Heatmap of Feature Importance Methods\",\n       x = \"\", y = \"\") +\n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))\n\n\n\n\n\n\n\n\n\nFinally, I visualized Gini, ROC AUC permutation, and Accuracy permutation importance all together in a faceted bar and correlation heatmap. The shape of the faceted graphs look very similar and all the values in the heatmap are greater than 0.86. This suggest that the most and less important features determined by all three methods are very much overlapped.\nFeatures like “tenure_months”, “total_charges”, and “contract.Month.to.month” likely dominate in all metrics.In contrast, features such as “phone_service”, “multiple_services”, “payment_method.Mailed.check”, and “payment_method.Bank.transfer.automatic” consistently appear at the bottom across all metrics.\nInterestingly, total_charges and tenure_months have higher importance than average_monthly_charge and monthly_charges across all three methods. This is a clear sign that long-term customers tend to have different churn behaviors than newer ones. Customers who have stayed longer or spent more over time may likely exhibit distinct patterns of loyalty or dissatisfaction.\nSimilarly, “contract.Month.to.month” ranks high in both Gini and Accuracy, underscoring its role in classification thresholds. It indicates that short-term contracts might make customers more flexible and prone to churn. However, it ranks slightly lower in ROC AUC, possibly because it contributes less to improve overall risk differentiation.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Tree-Based Models & Variable Importance</span>"
    ]
  },
  {
    "objectID": "conclusion.html",
    "href": "conclusion.html",
    "title": "4  Conclusion",
    "section": "",
    "text": "4.1 Citations:\nThe analysis reveals that although core features such as “tenure_months”, “total_charges”, and “contract.Month.to.month” are universally important in all approaches (expect PDP), striking differences appear in the way each method ranks importance.\nEven when there are high levels of overall correlation between approaches, the differences in rankings highlight the need to use several interpretation strategies to acquire a balanced view of model performance.\nhttps://statisticsglobe.com/cor-function-shows-only-na-and-one-in-r\nhttps://koalaverse.github.io/vip/articles/vip.html#a-classification-example\nhttps://www.kaggle.com/code/amycorona/predicting-churn-with-a-random-forest-classifier\nhttps://www.numberanalytics.com/blog/7-key-insights-area-under-roc-curve",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  }
]
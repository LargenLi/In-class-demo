[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Predicting Customer Churn",
    "section": "",
    "text": "1 Introduction",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "DA.html",
    "href": "DA.html",
    "title": "2  Data Cleaning & Feature Engineering",
    "section": "",
    "text": "2.1 Data Import and Initial Filtering\nCode\ndf = read_excel(\"Telco_customer_churn.xlsx\")\nCode\ncolnames(df) = gsub(\" \", \"_\", tolower(colnames(df)))\ndf = df |&gt; select(-c('churn_reason', 'customerid', 'count', 'country', 'city', 'zip_code', 'lat_long', 'latitude', 'longitude', 'churn_label', 'churn_score'))\nColumn names were standardized to lowercase with underscores. Irrelevant or redundant columns were removed, including identifiers (e.g., customerid), geolocation (e.g., latitude, longitude), and derived churn labels (e.g., churn_label). Since only churned row would have churn reason, the churn_reason is also dropped.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Cleaning & Feature Engineering</span>"
    ]
  },
  {
    "objectID": "DA.html#missing-value",
    "href": "DA.html#missing-value",
    "title": "2  Data Cleaning & Feature Engineering",
    "section": "2.2 Missing value",
    "text": "2.2 Missing value\n\n\nCode\nmissing_summary = sapply(df, function(x) sum(is.na(x)))\nprint(missing_summary)\n\n\n            state            gender    senior_citizen           partner \n                0                 0                 0                 0 \n       dependents     tenure_months     phone_service    multiple_lines \n                0                 0                 0                 0 \n internet_service   online_security     online_backup device_protection \n                0                 0                 0                 0 \n     tech_support      streaming_tv  streaming_movies          contract \n                0                 0                 0                 0 \npaperless_billing    payment_method   monthly_charges     total_charges \n                0                 0                 0                11 \n      churn_value              cltv \n                0                 0 \n\n\nCode\ndf = na.omit(df)\n\n\nRows with missing values are dropped.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Cleaning & Feature Engineering</span>"
    ]
  },
  {
    "objectID": "DA.html#feature-engineering",
    "href": "DA.html#feature-engineering",
    "title": "2  Data Cleaning & Feature Engineering",
    "section": "2.3 Feature Engineering",
    "text": "2.3 Feature Engineering\n\n\nCode\n# Average monthly charge\ndf = df |&gt;\n  mutate(avg_monthly_charge = ifelse(tenure_months == 0, 0, total_charges / tenure_months))\n\n# Multiple services\ndf = df |&gt;\n  mutate(multiple_services = ifelse((phone_service == \"Yes\" & internet_service != \"No\"), 1, 0))\n\n\nTwo new features (Average monthly charge & Multiple services) are created.\nThe variable avg_monthly_charge was created by dividing total_charges by tenure_months. This provides a time-normalized view of how much a customer spends on average per month.\nAnd multiple_services flags customers who are subscribed to both phone and internet services. This binary indicator captures service bundling, which is often a sign of higher customer commitment.\nSince this study is mainly focus on tree-based models and those models can handle multicollinearity naturally, I did not drop the original features.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Cleaning & Feature Engineering</span>"
    ]
  },
  {
    "objectID": "DA.html#binary-encoding-of-yesno-columns",
    "href": "DA.html#binary-encoding-of-yesno-columns",
    "title": "2  Data Cleaning & Feature Engineering",
    "section": "2.4 Binary Encoding of Yes/No Columns",
    "text": "2.4 Binary Encoding of Yes/No Columns\n\n\nCode\n# Columns with 'Yes' and 'No' value.\ndf = df |&gt;\n  mutate(across(where(is.character), ~ ifelse(str_starts(., \"No\"), \"No\", .)))\n\ncolnames(df) = str_trim(colnames(df))\n\nyes_no_cols = df |&gt;\n  select(where(is.character)) |&gt;\n  select(where(~ all(.x %in% c(\"Yes\", \"No\")))) |&gt;\n  colnames()\n\ndf = df |&gt;\n  mutate(across(all_of(yes_no_cols), ~ ifelse(. == \"Yes\", 1, 0)))\n\n\nTo prepare for models in the later section, all “No internet” or “No phone” values were simplified to “No”. Then, binary columns with “Yes”/“No” values were converted to 1/0 format.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Cleaning & Feature Engineering</span>"
    ]
  },
  {
    "objectID": "DA.html#factor-encoding-and-dummy-variables",
    "href": "DA.html#factor-encoding-and-dummy-variables",
    "title": "2  Data Cleaning & Feature Engineering",
    "section": "2.5 Factor Encoding and Dummy Variables",
    "text": "2.5 Factor Encoding and Dummy Variables\n\n\nCode\ndf = df |&gt;\n  mutate(across(where(is.character), as.factor))\n\ndf = df |&gt;\n  select(where(~ n_distinct(.) &gt; 1))\n\ndummies = dummyVars(~ ., data = df)\ndf_encoded = predict(dummies, newdata = df) |&gt; as.data.frame()\n\n\nRemaining categorical variables were converted to factors. Features with only one unique level were removed. All multi-class categorical variables were one-hot encoded using caret::dummyVars.\n\n\nCode\nnumeric_cols_to_scale = df_encoded |&gt;\n  select(where(is.numeric)) |&gt;\n  select(where(~ n_distinct(.) &gt; 2)) |&gt;\n  colnames()\n\n\npreproc = preProcess(df_encoded[, numeric_cols_to_scale], method = c(\"center\", \"scale\"))\nscaled_numeric = predict(preproc, df_encoded[, numeric_cols_to_scale])\n\ndf_final = df_encoded\ndf_final[, numeric_cols_to_scale] = scaled_numeric",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Cleaning & Feature Engineering</span>"
    ]
  },
  {
    "objectID": "DA.html#save-the-processed-dataset",
    "href": "DA.html#save-the-processed-dataset",
    "title": "2  Data Cleaning & Feature Engineering",
    "section": "2.6 Save the Processed Dataset",
    "text": "2.6 Save the Processed Dataset\n\n\nCode\n# write.csv(df_final, \"scaled_telco_data.csv\", row.names = FALSE)\n# write.csv(df_encoded, \"cleaned_telco_data.csv\", row.names = FALSE)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Cleaning & Feature Engineering</span>"
    ]
  },
  {
    "objectID": "Models.html",
    "href": "Models.html",
    "title": "3  Random Forest",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readr)\nlibrary(rpart)\nlibrary(rpart.plot)\nlibrary(pdp)\nlibrary(randomForest)\nlibrary(vip)\nlibrary(reshape2)\n\n\n\n\nCode\ndf = read_csv(\"cleaned_telco_data.csv\")\n\n\n\n\nCode\ntree_model = rpart(churn_value ~ ., data = df, method = \"class\", cp = 0.01)\n\nrpart.plot(tree_model, extra = 1)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Fit a Random Forest model\nset.seed(123)\ndf$churn_value = as.factor(df$churn_value)\n\ncolnames(df) = make.names(colnames(df))\nrf_model = randomForest(churn_value ~ ., data = df, importance = TRUE)\n\n\n\n\nCode\n# Get Gini importance from the RF model\ngini_df = as.data.frame(importance(rf_model)) |&gt;\n  rownames_to_column(\"Feature\") |&gt;\n  arrange(desc(MeanDecreaseGini))  # sort by importance\n\n# Plot with ggplot2\nggplot(gini_df, aes(x = reorder(Feature, MeanDecreaseGini), y = MeanDecreaseGini)) +\n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  labs(\n    title = \"Variable Importance (Gini Index)\",\n    x = \"Feature\",\n    y = \"Mean Decrease in Gini\"\n  ) +\n  theme_minimal(base_size = 12)\n\n\n\n\n\n\n\n\n\n\n\nCode\ntrain_x = df[, setdiff(names(df), \"churn_value\")]\n\nvar = colnames(train_x)\n\npdp_df = map(var, function(x)partial(rf_model,pred.var = x))|&gt;\n  map_dfr(pivot_longer, cols =1)\n\npdp_importance &lt;- pdp_df %&gt;%\n  group_by(name) %&gt;%\n  summarise(pdp_sd = sd(yhat), .groups = \"drop\")\n\n# Step 4: Join and reorder factor levels\npdp_df &lt;- pdp_df %&gt;%\n  left_join(pdp_importance, by = \"name\") %&gt;%\n  arrange(desc(pdp_sd))\n\n\n\n\nCode\n# Step 5: Plot\nggplot(pdp_df, aes(x = value, y = yhat)) +\n  geom_line(color = \"steelblue\") +\n  facet_wrap(~ name, scales = \"free_x\") +\n  labs(\n    title = \"Partial Dependence Plots Ordered by PDP Effect Size (sd of yhat)\",\n    x = \"Feature Value\",\n    y = \"Predicted Churn Probability\"\n  ) +\n  theme_minimal(base_size = 12)\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(pdp_importance, aes(x = reorder(name, pdp_sd), y = pdp_sd)) +\n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  labs(\n    title = \"Stdv. of y hat\",\n    x = \"Name\",\n    y = \"sd(yhat)\"\n  ) +\n  theme_minimal(base_size = 12)\n\n\n\n\n\n\n\n\n\n\n\nCode\nis_binary &lt;- function(x) length(unique(x)) == 2 && all(unique(x) %in% c(0, 1))\n\ncontinuous_vars &lt;- train_x %&gt;%\n  select(where(~ !is_binary(.))) %&gt;%\n  colnames()\n\ncon_df = map(continuous_vars, function(x)partial(rf_model,pred.var = x))|&gt;\n  map_dfr(pivot_longer, cols =1)\n\ncon_importance &lt;- con_df %&gt;%\n  group_by(name) %&gt;%\n  summarise(pdp_sd = sd(yhat), .groups = \"drop\")\n\n# Step 4: Join and reorder factor levels\ncon_df &lt;- con_df %&gt;%\n  left_join(con_importance, by = \"name\") %&gt;%\n  arrange(desc(pdp_sd))\n\n\n\n\nCode\n# Step 5: Plot\nggplot(con_df, aes(x = value, y = yhat)) +\n  geom_line(color = \"steelblue\") +\n  facet_wrap(~ name, scales = \"free_x\") +\n  labs(\n    title = \"Partial Dependence Plots for Contin\",\n    x = \"Feature Value\",\n    y = \"Predicted Churn Probability\"\n  ) +\n  theme_minimal(base_size = 12)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Define prediction wrapper to return predicted class labels\npred_class = function(object, newdata) {\n  factor(predict(object, newdata = newdata, type = \"response\"), levels = c(0, 1))\n}\n\n# Generate VIP plot using accuracy\nvip(rf_model,\n    method = \"permute\",\n    train = train_x,                \n    target = df$churn_value,           \n    metric = \"accuracy\",               \n    pred_wrapper = pred_class,        \n    nsim = 10,                  \n    num_features = ncol(train_x))     \n\n\n\n\n\n\n\n\n\n\n\nCode\n# Generate VIP plot using roc\nvip(rf_model,\n    method = \"permute\",\n    train = train_x,\n    target = df$churn_value,\n    metric = \"roc_auc\",\n    pred_wrapper = function(object, newdata) {\n      predict(object, newdata = newdata, type = \"prob\")[, \"1\"]\n    },\n    nsim = 10,\n    num_features = ncol(train_x))\n\n\n\n\n\n\n\n\n\n\n\nCode\ntrain_x = df[, setdiff(names(df), \"churn_value\")]\n\n# Generate VIP plot using roc\nvip(rf_model,\n    method = \"permute\",\n    train = train_x,\n    target = df$churn_value,\n    metric = \"roc_auc\",\n    event_level = \"second\",\n    pred_wrapper = function(object, newdata) {\n      predict(object, newdata = newdata, type = \"prob\")[, \"1\"]\n    },\n    nsim = 10,\n    num_features = ncol(train_x))\n\n\n\n\n\n\n\n\n\n\n\nCode\n# ---- Gini Importance ----\ngini_df = as.data.frame(importance(rf_model)) |&gt;\n  rownames_to_column(\"Feature\") |&gt;\n  select(Feature, Gini = MeanDecreaseGini)\n\n# ---- ROC AUC Permutation ----\nroc_vi = vi(\n  rf_model,\n  method = \"permute\",\n  train = train_x,\n  target = df$churn_value,\n  metric = \"roc_auc\",\n  event_level = \"second\",\n  pred_wrapper = function(object, newdata) {\n    predict(object, newdata = newdata, type = \"prob\")[, \"1\"]\n  },\n  nsim = 10\n) |&gt; select(Feature = Variable, ROC_AUC = Importance)\n\n# ---- Accuracy Permutation ----\npred_class = function(object, newdata) {\n  factor(predict(object, newdata, type = \"response\"), levels = c(0, 1))\n}\n\nacc_vi = vi(\n  rf_model,\n  method = \"permute\",\n  train = train_x,\n  target = df$churn_value,\n  metric = \"accuracy\",\n  pred_wrapper = pred_class,\n  nsim = 10\n) |&gt; select(Feature = Variable, Accuracy = Importance)\n\n# ---- Merge All ----\nvi_comparison = reduce(list(gini_df, roc_vi, acc_vi), full_join, by = \"Feature\") |&gt;\n  arrange(desc(Gini))\n\n\n\n\nCode\n# ---- Plot ----\nvi_long = vi_comparison |&gt;\n  pivot_longer(cols = -Feature, names_to = \"Metric\", values_to = \"Importance\")\n\n\n\n\nCode\nggplot(vi_long, aes(x = reorder(Feature, Importance), y = Importance)) +\n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  facet_wrap(~ Metric, scales = \"free_x\") +\n  labs(\n    title = \"Feature Importance by Metric\",\n    x = \"Feature\",\n    y = \"Importance\"\n  ) +\n  theme_minimal(base_size = 12)\n\n\n\n\n\n\n\n\n\n\n\nCode\nvi_corr = vi_comparison |&gt;\n  select(-Feature) |&gt;\n  cor(use = \"complete.obs\")\n\n# Convert to long format for ggplot2\nvi_corr_long = melt(vi_corr)\n\n# Plot heatmap\nggplot(vi_corr_long, aes(x = Var1, y = Var2, fill = value)) +\n  geom_tile(color = \"white\") +\n  geom_text(aes(label = round(value, 2)), color = \"black\", size = 5) +\n  scale_fill_gradient2(low = \"steelblue\", high = \"darkred\", mid = \"white\",\n                       midpoint = 0.5, limit = c(0, 1), space = \"Lab\",\n                       name = \"Correlation\") +\n  theme_minimal(base_size = 14) +\n  labs(title = \"Correlation of Feature Importance Metrics\",\n       x = \"\", y = \"\") +\n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Random Forest</span>"
    ]
  }
]